
###############################################
# Exemplo de .env para Participa DF Backend   #
###############################################

# Token de autenticação Hugging Face (OBRIGATÓRIO para LLAMA Árbitro funcionar)
# Obtenha em: https://huggingface.co/settings/tokens
# O token precisa ter acesso ao modelo (aceitar termos de uso do Llama)
HF_TOKEN=seu_token_huggingface_aqui

# Modelo LLM para arbitragem (opcional - usa Llama-3.2-3B por padrão)
# Modelos disponíveis: meta-llama/Llama-3.2-3B-Instruct, meta-llama/Llama-3.1-70B-Instruct
# HF_MODEL=meta-llama/Llama-3.2-3B-Instruct

# Configuração do LLAMA Árbitro (Ativação Inteligente)
# Desativado por padrão, mas acionado AUTOMATICAMENTE em ambiguidades
# Casos claros (confiança alta) = sem LLM (gratuito + rápido)
# Casos ambíguos (0.5-0.8, apenas nomes, >=3 baixa confiança) = LLM automático ($$)
# Ative globalmente com True se preferir forçar LLM em todas as análises
PII_USE_LLM_ARBITRATION=False

# Usar GPU para modelos NER (se disponível)
PII_USAR_GPU=True

# Configuração do Celery/Redis (processamento em lote)
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Instruções:
# 1. Renomeie este arquivo para .env
# 2. Obtenha HF_TOKEN em https://huggingface.co/settings/tokens
# 3. Aceite os termos de uso do Llama em huggingface.co/meta-llama
# 4. O sistema carrega automaticamente o .env em todos os entrypoints do backend
# 5. O LLM é acionado AUTOMATICAMENTE em ambiguidades
# 6. Para forçar LLM em TUDO: PII_USE_LLM_ARBITRATION=True
# 6. Sem HF_TOKEN válido, o sistema usa fallback local (ensemble sem árbitro LLM)